#!/bin/bash
#SBATCH --job-name=py4dstem
#SBATCH --mail-user=cl2696@cornell.edu       # Where to send mail
#SBATCH --nodes=1                            # number of nodes requested
#SBATCH --ntasks=1                           # number of tasks to run in parallel
#SBATCH --cpus-per-task=32                    # number of CPUs required for each task. 4 for 10GB, 8 for 20GB, 32 for 80GB of A100.
#SBATCH --gres=gpu:a100:4                 # request a GPU #gpu:a100:1, gpu:2g.20gb:1
#SBATCH --nodelist=c0001
#SBATCH --time=336:00:00                     # Time limit hrs:min:sec
#SBATCH --output=03_output/logs/log_job_%j_py4dstem_convergence_tBL_WSe2_p12_6slice.txt  # Standard output and error log to /logs, you need to create this folder first!

pwd; hostname; date

module load cuda/11.8

source activate py4DSTEM

## Set the params_path variable
PARAMS_PATH="01_params/py4dstem_convergence_tBL_WSe2_p12_6slice.yml"
echo params_path = ${PARAMS_PATH}

## Making sure we are under repo root and calling `sbatch scripts/slurm_run_py4dstem.sub`
cd ~/workspace/ptyrad_paper; pwd; 

python -u ./02_scripts/run_py4dstem.py --params_path "${PARAMS_PATH}" 2>&1

date