#!/bin/bash
#SBATCH --job-name=ptyrad
#SBATCH --mail-user=cl2696@cornell.edu       # Where to send mail
#SBATCH --nodes=1                            # number of nodes requested
#SBATCH --ntasks=1                           # number of tasks to run in parallel
#SBATCH --cpus-per-task=32                    # number of CPUs required for each task. 4 for 10GB, 8 for 20GB, 32 for 80GB of A100.
#SBATCH --gres=gpu:a100:1                 # request a GPU #gpu:a100:1, gpu:2g.20gb:1
#SBATCH --time=336:00:00                     # Time limit hrs:min:sec
#SBATCH --output=03_output/logs/log_job_%j_ptyrad_convergence_tBL_WSe2_p12_6slice.txt  # Standard output and error log

pwd; hostname; date

module load cuda/11.8

source activate ptyrad_paper # ptyrad install from pip

## Set the params_path variable
PARAMS_PATH="01_params/ptyrad_convergence_tBL_WSe2_p12_6slice.yml"
echo params_path = ${PARAMS_PATH}

## Making sure we are under repo root and calling `sbatch scripts/slurm_run_ptyrad.sub`
cd ~/workspace/ptyrad_paper; pwd; 

## The gpuid is used to assign the device for PtyRAD, it can be either 'acc', 'cpu', or an integer
## The jobid is used as a unique identifier for hypertune mode with multiple GPU workers on different nodes. 
## The JOBID is an environment variable that'll be automatically set to 1-N via LoopSubmit.sh. If not set, default to 0.

python -u ./02_scripts/run_ptyrad_loop.py --params_path "${PARAMS_PATH}" 2>&1

date